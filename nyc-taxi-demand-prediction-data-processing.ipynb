{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f21b961",
   "metadata": {
    "papermill": {
     "duration": 0.019413,
     "end_time": "2022-04-04T20:41:04.538404",
     "exception": false,
     "start_time": "2022-04-04T20:41:04.518991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Newyork city Taxi Demand Prediction Data Processing and modeling\n",
    "\n",
    "   Please review and visit  [NYC Taxi Demand EDA](https://www.kaggle.com/chiragtagadiya/nyc-taxi-demand-prediction-eda/edit) for Data Analysis.\n",
    "   \n",
    "   This Notebook cotains data preprocessing and Data Modeling\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c90fb",
   "metadata": {
    "papermill": {
     "duration": 0.017618,
     "end_time": "2022-04-04T20:41:04.576040",
     "exception": false,
     "start_time": "2022-04-04T20:41:04.558422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features in the dataset:\n",
    "<table border=\"1\">\n",
    "        <tr>\n",
    "            <th>Field Name</th>\n",
    "            <th>Description</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>VendorID</td>\n",
    "            <td>\n",
    "            A code indicating the TPEP provider that provided the record.\n",
    "            <ol>\n",
    "                <li>Creative Mobile Technologies</li>\n",
    "                <li>VeriFone Inc.</li>\n",
    "            </ol>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>tpep_pickup_datetime</td>\n",
    "            <td>The date and time when the meter was engaged.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>tpep_dropoff_datetime</td>\n",
    "            <td>The date and time when the meter was disengaged.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Passenger_count</td>\n",
    "            <td>The number of passengers in the vehicle. This is a driver-entered value.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Trip_distance</td>\n",
    "            <td>The elapsed trip distance in miles reported by the taximeter.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Pickup_longitude</td>\n",
    "            <td>Longitude where the meter was engaged.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Pickup_latitude</td>\n",
    "            <td>Latitude where the meter was engaged.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>RateCodeID</td>\n",
    "            <td>The final rate code in effect at the end of the trip.\n",
    "            <ol>\n",
    "                <li> Standard rate </li>\n",
    "                <li> JFK </li>\n",
    "                <li> Newark </li>\n",
    "                <li> Nassau or Westchester</li>\n",
    "                <li> Negotiated fare </li>\n",
    "                <li> Group ride</li>\n",
    "            </ol>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Store_and_fwd_flag</td>\n",
    "            <td>This flag indicates whether the trip record was held in vehicle memory before sending to the vendor,<br> aka “store and forward,” because the vehicle did not have a connection to the server.<br><br>\n",
    "                Y= store and forward trip<br>\n",
    "                N= not a store and forward trip<br>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Dropoff_longitude</td>\n",
    "            <td>Longitude where the meter was disengaged.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Dropoff_ latitude</td>\n",
    "            <td>Latitude where the meter was disengaged.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Payment_type</td>\n",
    "            <td>A numeric code signifying how the passenger paid for the trip.\n",
    "            <ol>\n",
    "                <li> Credit card </li>\n",
    "                <li> Cash </li>\n",
    "                <li> No charge </li>\n",
    "                <li> Dispute</li>\n",
    "                <li> Unknown </li>\n",
    "                <li> Voided trip</li>\n",
    "            </ol>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Fare_amount</td>\n",
    "            <td>The time-and-distance fare calculated by the meter.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Extra</td>\n",
    "            <td>Miscellaneous extras and surcharges. Currently, this only includes. the $0.50 and $1 rush hour and overnight charges.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>MTA_tax</td>\n",
    "            <td>0.50 MTA tax that is automatically triggered based on the metered rate in use.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Improvement_surcharge</td>\n",
    "            <td>0.30 improvement surcharge assessed trips at the flag drop. the improvement surcharge began being levied in 2015.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Tip_amount</td>\n",
    "            <td>Tip amount – This field is automatically populated for credit card tips.Cash tips are not included.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Tolls_amount</td>\n",
    "            <td>Total amount of all tolls paid in trip.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Total_amount</td>\n",
    "            <td>The total amount charged to passengers. Does not include cash tips.</td>\n",
    "        </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c1cd52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:04.616106Z",
     "iopub.status.busy": "2022-04-04T20:41:04.614880Z",
     "iopub.status.idle": "2022-04-04T20:41:07.047470Z",
     "shell.execute_reply": "2022-04-04T20:41:07.046577Z",
     "shell.execute_reply.started": "2022-04-04T20:01:32.618823Z"
    },
    "papermill": {
     "duration": 2.453613,
     "end_time": "2022-04-04T20:41:07.047625",
     "exception": false,
     "start_time": "2022-04-04T20:41:04.594012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd#similar to pandas\n",
    "\n",
    "import pandas as pd#pandas to create small dataframes \n",
    "\n",
    "#!pip3 install folium\n",
    "import folium #open street map\n",
    "\n",
    "# unix time: https://www.unixtimestamp.com/\n",
    "import datetime #Convert to unix time\n",
    "\n",
    "import time #Convert to unix time\n",
    "\n",
    "import numpy as np#Do aritmetic operations on arrays\n",
    "\n",
    "# matplotlib: used to plot graphs\n",
    "import matplotlib\n",
    "# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user intractive like zoom in and zoom out\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns#Plots\n",
    "from matplotlib import rcParams#Size of plots  \n",
    "\n",
    "# this lib is used while we calculate the stight line distance between two (lat,lon) pairs in miles\n",
    "#!pip install gpxpy\n",
    "import gpxpy.geo #Get the haversine distance\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "# to install xgboost: \n",
    "# !conda install py-xgboost --yes\n",
    "#!pip3 install xgboost\n",
    "# if it didnt happen check install_xgboost.JPG\n",
    "import xgboost as xgb\n",
    "\n",
    "# to install sklearn: pip install -U scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fdd0c",
   "metadata": {
    "papermill": {
     "duration": 0.018088,
     "end_time": "2022-04-04T20:41:07.083965",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.065877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Processing Pipiline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b589b162",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.126183Z",
     "iopub.status.busy": "2022-04-04T20:41:07.125256Z",
     "iopub.status.idle": "2022-04-04T20:41:07.128097Z",
     "shell.execute_reply": "2022-04-04T20:41:07.127584Z"
    },
    "papermill": {
     "duration": 0.026012,
     "end_time": "2022-04-04T20:41:07.128239",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.102227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_timestamp(s):\n",
    "    return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d1cfea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.174204Z",
     "iopub.status.busy": "2022-04-04T20:41:07.172578Z",
     "iopub.status.idle": "2022-04-04T20:41:07.176375Z",
     "shell.execute_reply": "2022-04-04T20:41:07.176870Z"
    },
    "papermill": {
     "duration": 0.030569,
     "end_time": "2022-04-04T20:41:07.177061",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.146492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Returned Columns:\n",
    "    # 1. passenger_count\n",
    "    # 2. trip_distance\n",
    "    # 3. pickup_longitude\n",
    "    # 4. dropoff_latitude\n",
    "    # 5. dropoff_longitude\n",
    "    # 6. pickup_latitude\n",
    "    # 7. total_amount : total paid fair amount\n",
    "    # 8. trip_times :  duration of each trip\n",
    "    # 9. pickup_times : pickup time converted into unix time \n",
    "    # 10.drop times: drop time converted to unix time\n",
    "    # 11.Speed : velocity of each trip\n",
    "\"\"\"\n",
    "\n",
    "def create_dataframe_with_trip_times(df):\n",
    "    \n",
    "    duration_df = df[['tpep_pickup_datetime','tpep_dropoff_datetime']].compute()\n",
    "    # get pickup values in unix time\n",
    "    pickup_times = [convert_to_unix_timestamp(value) for value in duration_df['tpep_pickup_datetime'].values]\n",
    "    # get dropoff values in unix time\n",
    "    drop_times = [convert_to_unix_timestamp(value) for value in duration_df['tpep_dropoff_datetime'].values]\n",
    "    # drop times and pickup times devide by 60 will give minutes\n",
    "    trip_times = (np.array(drop_times) - np.array(pickup_times))/ float(60) \n",
    "    result_df = df[['passenger_count','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','total_amount']].compute()\n",
    "    \n",
    "    result_df['trip_times'] = trip_times\n",
    "    result_df['pickup_times'] = pickup_times\n",
    "    result_df['drop_times'] = drop_times\n",
    "    # velocity = distance/ duration\n",
    "    result_df['Speed'] = 60*(result_df['trip_distance']/result_df['trip_times'])\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8678f3d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.218270Z",
     "iopub.status.busy": "2022-04-04T20:41:07.217248Z",
     "iopub.status.idle": "2022-04-04T20:41:07.231584Z",
     "shell.execute_reply": "2022-04-04T20:41:07.232085Z"
    },
    "papermill": {
     "duration": 0.036459,
     "end_time": "2022-04-04T20:41:07.232275",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.195816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    print(\"Number of records in dataframe : {}\".format(df.shape[0]))\n",
    "    # remove Latitude and longitude errors and outliers\n",
    "    a = df.shape[0]\n",
    "    temp_frame = df[((df.dropoff_longitude >= -74.15) & (df.dropoff_longitude <= -73.7004) &\\\n",
    "                       (df.dropoff_latitude >= 40.5774) & (df.dropoff_latitude <= 40.9176)) & \\\n",
    "                       ((df.pickup_longitude >= -74.15) & (df.pickup_latitude >= 40.5774)& \\\n",
    "                       (df.pickup_longitude <= -73.7004) & (df.pickup_latitude <= 40.9176))]\n",
    "    b = temp_frame.shape[0]\n",
    "    print (\"Number of outlier coordinates lying outside NY boundaries:\",(a-b))\n",
    "    \n",
    "    \n",
    "    temp_frame = df[(df.trip_times > 0) & (df.trip_times < 720)]\n",
    "    c = temp_frame.shape[0]\n",
    "    print (\"Number of outliers from trip times analysis:\",(a-c))\n",
    "    \n",
    "    temp_frame = df[(df.Speed <= 65) & (df.Speed >= 0)]\n",
    "    e = temp_frame.shape[0]\n",
    "    print (\"Number of outliers from speed analysis:\",(a-e))\n",
    "    \n",
    "    temp_frame = df[(df.total_amount <1000) & (df.total_amount >0)]\n",
    "    f = temp_frame.shape[0]\n",
    "    print (\"Number of outliers from fare analysis:\",(a-f))\n",
    "\n",
    "    del temp_frame\n",
    "    \n",
    "    df = df[((df.dropoff_longitude >= -74.15) & (df.dropoff_longitude <= -73.7004) &\\\n",
    "                       (df.dropoff_latitude >= 40.5774) & (df.dropoff_latitude <= 40.9176)) & \\\n",
    "                       ((df.pickup_longitude >= -74.15) & (df.pickup_latitude >= 40.5774)& \\\n",
    "                       (df.pickup_longitude <= -73.7004) & (df.pickup_latitude <= 40.9176))]\n",
    "    \n",
    "    df = df[(df.trip_times > 0) & (df.trip_times < 720)]\n",
    "    df = df[(df.trip_distance > 0) & (df.trip_distance < 23)]\n",
    "    df = df[(df.Speed < 45.31) & (df.Speed > 0)]\n",
    "    df = df[(df.total_amount <1000) & (df.total_amount >0)]\n",
    "    \n",
    "    print (\"Total outliers removed\",a - df.shape[0])\n",
    "    print (\"---\"*50)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e670f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.273788Z",
     "iopub.status.busy": "2022-04-04T20:41:07.272719Z",
     "iopub.status.idle": "2022-04-04T20:41:07.278624Z",
     "shell.execute_reply": "2022-04-04T20:41:07.279095Z"
    },
    "papermill": {
     "duration": 0.028443,
     "end_time": "2022-04-04T20:41:07.279272",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.250829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_regions(k, coords, frame_with_outliers_removed):\n",
    "    \"\"\"\n",
    "    Get the Cluster centroid and length of clusters, \n",
    "    Devide the NYC city into Different clusters based on number of pickups\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "        k = number of clusters for Kmeans algorithm\n",
    "        coords =   Trip pick up location Latitude and Longitud  dataframe on which we want to clusters,\n",
    "        frame_with_outliers_removed = dataframe on which we want to fit clustering algorithm\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "        Number of region centroid\n",
    "    \n",
    "    \"\"\"\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=10000,random_state=42).fit(coords)\n",
    "    # predict the cluster and create new feature pickup clusters\n",
    "    frame_with_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_outliers_removed[['pickup_latitude', 'pickup_longitude']])\n",
    "    cluster_centroids = kmeans.cluster_centers_\n",
    "    cluster_len = len(cluster_centroids)\n",
    "    return cluster_centroids, cluster_len, frame_with_outliers_removed\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a224dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.319892Z",
     "iopub.status.busy": "2022-04-04T20:41:07.319211Z",
     "iopub.status.idle": "2022-04-04T20:41:07.329791Z",
     "shell.execute_reply": "2022-04-04T20:41:07.330372Z"
    },
    "papermill": {
     "duration": 0.032705,
     "end_time": "2022-04-04T20:41:07.330559",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.297854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cluster_statistics(cluser_centroids, cluster_len):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the avg number  cluster with inter-cluster distance > 2\n",
    "    Get the avg number  cluster with inter-cluster distance < 2\n",
    "    Get the minimum inter-cluster distance ( our constraint for good clustering is 0.5 miles)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "        cluser_centroids = centroid of each cluster in city\n",
    "        cluster_len =  total number of clusters\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"Choosing the cluster of size : {} \".format(cluster_len))\n",
    "    less_than_2_miles=[]\n",
    "    more_than_2_miles=[]\n",
    "    # set min_dist with some large value (1000 miles)\n",
    "    min_dist = 1000\n",
    "    for i in  range(0,cluster_len) :\n",
    "        good_points=0\n",
    "        violated_points=0\n",
    "        for j in range(0, cluster_len):\n",
    "            if i != j :\n",
    "                # movable-type.co.uk/scripts/latlong.html\n",
    "                # haversine distance\n",
    "                centroid1 = cluser_centroids[i]\n",
    "                centroid2 = cluser_centroids[j]\n",
    "                distance_bw_centroids_in_meters = gpxpy.geo.haversine_distance(centroid1[0], centroid1[1],centroid2[0], centroid2[1])\n",
    "                # haversine_distance will be in meters, so convert distance into miles  by deviding with 1.60934*1000.\n",
    "                distance_bw_centroids_in_miles = distance_bw_centroids_in_meters/(1.60934*1000)\n",
    "                min_dist = min(min_dist,distance_bw_centroids_in_miles)\n",
    "                \n",
    "                if distance_bw_centroids_in_miles <=2:\n",
    "                    good_points+=1\n",
    "                else:\n",
    "                    violated_points+=1\n",
    "                    \n",
    "        less_than_2_miles.append(good_points)\n",
    "        more_than_2_miles.append(violated_points)\n",
    "        \n",
    "    \n",
    "    print(\"Final Result After Computation:\")\n",
    "    print(\"Avg Number of cluster with in vicinity [inter-cluster distance is < 2]  : {}\".format(np.ceil(sum(less_than_2_miles)/len(less_than_2_miles))))\n",
    "    print(\"Avg Number of cluster outside  vicinity [inter-cluster distance is >  2] : {}\".format(np.ceil(sum(more_than_2_miles)/len(more_than_2_miles))))\n",
    "    print(\"Minimum inter-cluster distance : {}\".format(min_dist))\n",
    "    print(\"+\"*70)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c210e870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.372163Z",
     "iopub.status.busy": "2022-04-04T20:41:07.371458Z",
     "iopub.status.idle": "2022-04-04T20:41:07.378927Z",
     "shell.execute_reply": "2022-04-04T20:41:07.379566Z"
    },
    "papermill": {
     "duration": 0.029885,
     "end_time": "2022-04-04T20:41:07.379729",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.349844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Refer:https://www.unixtimestamp.com/\n",
    "\n",
    "# 1420070400 : 2015-01-01 00:00:00 \n",
    "# 1422748800 : 2015-02-01 00:00:00 \n",
    "# 1425168000 : 2015-03-01 00:00:00\n",
    "# 1427846400 : 2015-04-01 00:00:00 \n",
    "# 1430438400 : 2015-05-01 00:00:00 \n",
    "# 1433116800 : 2015-06-01 00:00:00\n",
    "\n",
    "# 1451606400 : 2016-01-01 00:00:00 \n",
    "# 1454284800 : 2016-02-01 00:00:00 \n",
    "# 1456790400 : 2016-03-01 00:00:00\n",
    "# 1459468800 : 2016-04-01 00:00:00 \n",
    "# 1462060800 : 2016-05-01 00:00:00 \n",
    "# 1464739200 : 2016-06-01 00:00:00\n",
    "\n",
    "\n",
    "\n",
    "def add_pickup_bins(frame,month,year):\n",
    "    \"\"\"\n",
    "    Devide all the trips in 10 minute bins    \n",
    "    \"\"\"\n",
    "    unix_pickup_times=[i for i in frame['pickup_times'].values]\n",
    "    #     unix_pickup_times = [i for i in frame['pickup_times'].values]\n",
    "    # index = 0 Jan_2015 Feb_2015 Mar_2015 Apr_2015 May_2015 June_2015\n",
    "    # index = 1 Jan_2015 Feb_2015 Mar_2015 Apr_2015 May_2015 June_2015\n",
    "    \n",
    "    unix_times = [[1420070400,1422748800,1425168000,1427846400,1430438400,1433116800],\\\n",
    "                    [1451606400,1454284800,1456790400,1459468800,1462060800,1464739200]]\n",
    "    \n",
    "    start_pickup_unix = unix_times[year-2015][month-1]\n",
    "    \n",
    "    # time.mktime will return timestamp in local time \n",
    "    # we will  subtract 24 ( 4 hours = 4*60*60 = 14400 seconds = 14400/600\n",
    "    # devide by 600 for 10 minutes inverval setting) =  24 \n",
    "    # if you are in Indian standard time then correction would be + 33 ,\n",
    "    # because India is GMT +5:30, 5.30*60*60 = 19800 => 19800/600 for 10 minutes bin we get +33.\n",
    "    \n",
    "    #     ten_minute_wise_binned_unix_pickup_times = [ int((i - start_pickup_unix )/600)+33  for i in unix_pickup_times]\n",
    "    #  I am in Eastern time so no correction needed. \n",
    "    correction = 0\n",
    "    ten_minute_wise_binned_unix_pickup_times=[(int((i-start_pickup_unix)/600) + correction) for i in unix_pickup_times]\n",
    "    frame['pickup_bins']=np.array(ten_minute_wise_binned_unix_pickup_times)\n",
    "    return frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57cea48e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.428383Z",
     "iopub.status.busy": "2022-04-04T20:41:07.427678Z",
     "iopub.status.idle": "2022-04-04T20:41:07.430338Z",
     "shell.execute_reply": "2022-04-04T20:41:07.430866Z"
    },
    "papermill": {
     "duration": 0.031634,
     "end_time": "2022-04-04T20:41:07.431077",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.399443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upto now we cleaned data and prepared data for the month 2015,\n",
    "\n",
    "# now do the same operations for months Jan, Feb, March of 2016\n",
    "\n",
    "def dataPreparation(df,month, year, record_count ):\n",
    "    \"\"\"\n",
    "        Prepare final data.\n",
    "\n",
    "        1. get the correct column from dataframe\n",
    "        2. compute trip duration features\n",
    "        3. computer speed, unix time stamp for pickups, remove outliers\n",
    "        4. add pickup cluster for each trip\n",
    "        5. add pickup bin for each trip\n",
    "        6. groupby trip cluster, trip bin, and trip_distance \n",
    "        7. return prepared dataframe.\n",
    "    \"\"\"\n",
    "    # 1. get the correct column from dataframe 2. compute trip duration features\n",
    "    print(\"Adding Trip times for each trip \")\n",
    "    \n",
    "    df = create_dataframe_with_trip_times(df)\n",
    "    \n",
    "    # 3. computer speed, unix time stamp for pickups, remove outliers\n",
    "    print(\"Removing outliers from dataframe\")\n",
    "    \n",
    "    df = remove_outliers(df)\n",
    "    print(\"fraction of data points that remain after removing outliers\", float(len(df)/record_count))\n",
    "    \n",
    "    # 4. add pickup cluster for each trip\n",
    "    print(\"Creating Trip clusters\")\n",
    "    \n",
    "    # We have figure it out optimal value for kmeans clustering to 35 in eda section\n",
    "    \n",
    "    _,a, df = find_regions(35, df[['pickup_latitude', 'pickup_longitude']], df)\n",
    "    \n",
    "    print(\"Adding trip in 10 minute time interval bins\")\n",
    "    # 5.add pickup bin for each trip\n",
    "    df = add_pickup_bins(df,month,year)\n",
    "    \n",
    "    print(\"Group by trip cluster and then trip bin\")\n",
    "    # 6.groupby trip cluster, trip bin, and trip_distance \n",
    "    df_groupby = df[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()\n",
    "    \n",
    "    return df_groupby, df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5449d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.475295Z",
     "iopub.status.busy": "2022-04-04T20:41:07.474607Z",
     "iopub.status.idle": "2022-04-04T20:41:07.478668Z",
     "shell.execute_reply": "2022-04-04T20:41:07.479200Z"
    },
    "papermill": {
     "duration": 0.028809,
     "end_time": "2022-04-04T20:41:07.479368",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.450559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data files are extremely  large, more than 10 millions records\n",
    "# We already processed and saved processed file\n",
    "# So If you want to reproduce this uncomment below line\n",
    "\n",
    "# month_jan_2016 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2016-01.csv')\n",
    "# month_feb_2016 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2016-02.csv')\n",
    "# month_mar_2016 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2016-03.csv')\n",
    "# month_jan_2015 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2015-01.csv')\n",
    "\n",
    "# jan_2015_groupby,jan_2015_frame = dataPreparation(month_jan_2015,1,2015,len(month_jan_2015))\n",
    "# jan_2016_groupby,jan_2016_frame = dataPreparation(month_jan_2016,1,2016,len(month_jan_2016))\n",
    "\n",
    "# feb_2016_groupby,feb_2016_frame = dataPreparation(month_feb_2016,2,2016,len(month_feb_2016))\n",
    "\n",
    "\n",
    "# mar_2016_groupby,mar_2016_frame = dataPreparation(month_mar_2016,3,2016,len(month_mar_2016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083fed6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.521179Z",
     "iopub.status.busy": "2022-04-04T20:41:07.520560Z",
     "iopub.status.idle": "2022-04-04T20:41:07.731302Z",
     "shell.execute_reply": "2022-04-04T20:41:07.731889Z",
     "shell.execute_reply.started": "2022-04-04T20:36:25.507499Z"
    },
    "papermill": {
     "duration": 0.23319,
     "end_time": "2022-04-04T20:41:07.732083",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.498893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "feb_2016_frame = dd.read_csv('../input/preprocessing-files/feb_2016_processed.csv')\n",
    "feb_2016_group_frame = dd.read_csv('../input/preprocessing-files/feb_2016_groupby.csv')\n",
    "jan_2015_frame  = dd.read_csv('../input/preprocessing-files/jan_month_2015.csv')\n",
    "jan_2015_group_frame = dd.read_csv('../input/preprocessing-files/jan_2015_groupby.csv')\n",
    "jan_2016_group_frame = dd.read_csv('../input/preprocessing-files/jan_2016_groupby.csv')\n",
    "jan_2016_frame  = dd.read_csv('../input/preprocessing-files/jan_2016_processed.csv')\n",
    "mar_2016_frame  = dd.read_csv('../input/preprocessing-files/mar_2016_frame.csv')\n",
    "mar_2016_group_frame = dd.read_csv('../input/preprocessing-files/mar_2016_groupby.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18739a94",
   "metadata": {
    "papermill": {
     "duration": 0.019204,
     "end_time": "2022-04-04T20:41:07.770745",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.751541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Smoothing\n",
    "\n",
    "\n",
    "* There are **some bins in dataset having 0 pickups**, we want to findout those bins and smooth the dataset, 0 values in pickups is problamatic for ratio features, we will smooth our dataset to overcome this issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e7d95cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.813307Z",
     "iopub.status.busy": "2022-04-04T20:41:07.812647Z",
     "iopub.status.idle": "2022-04-04T20:41:07.817975Z",
     "shell.execute_reply": "2022-04-04T20:41:07.818523Z",
     "shell.execute_reply.started": "2022-04-04T20:35:02.207303Z"
    },
    "papermill": {
     "duration": 0.028597,
     "end_time": "2022-04-04T20:41:07.818689",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.790092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gets the unique bins where pickup values are present for each each reigion\n",
    "\n",
    "# for each cluster region we will collect all the indices of 10min intravels in which the pickups are happened\n",
    "# we got an observation that there are some pickpbins that doesnt have any pickups\n",
    "def get_unique_pickup_bins(df, cluster_count = 35):\n",
    "    \"\"\"\n",
    "    For each cluster in dataset, find the number of unique time bins present in each cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    values = []\n",
    "    for i in range(0,cluster_count):\n",
    "        new = df[df['pickup_cluster'] == i]\n",
    "        list_unq = list(set(new['pickup_bins']))\n",
    "        list_unq.sort()\n",
    "        values.append(list_unq)\n",
    "    return values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f306fd31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:07.861723Z",
     "iopub.status.busy": "2022-04-04T20:41:07.861053Z",
     "iopub.status.idle": "2022-04-04T20:41:15.735358Z",
     "shell.execute_reply": "2022-04-04T20:41:15.734593Z",
     "shell.execute_reply.started": "2022-04-04T20:38:25.771728Z"
    },
    "papermill": {
     "duration": 7.897236,
     "end_time": "2022-04-04T20:41:15.735553",
     "exception": false,
     "start_time": "2022-04-04T20:41:07.838317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time 7.865978717803955\n"
     ]
    }
   ],
   "source": [
    "# for every month we get all indices of 10min intravels in which atleast one pickup got happened\n",
    "\n",
    "#jan\n",
    "start = time.time() \n",
    "jan_2015_unique = get_unique_pickup_bins(jan_2015_group_frame)\n",
    "\n",
    "\n",
    "#jan 2016\n",
    "jan_2016_unique = get_unique_pickup_bins(jan_2016_group_frame)\n",
    "\n",
    "# #feb\n",
    "feb_2016_unique = get_unique_pickup_bins(feb_2016_group_frame)\n",
    "\n",
    "# #march\n",
    "mar_2016_unique = get_unique_pickup_bins(mar_2016_group_frame)\n",
    "end = time.time()\n",
    "print(\"Total processing time\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6cff02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T20:41:15.780087Z",
     "iopub.status.busy": "2022-04-04T20:41:15.779390Z",
     "iopub.status.idle": "2022-04-04T20:41:15.806175Z",
     "shell.execute_reply": "2022-04-04T20:41:15.805537Z",
     "shell.execute_reply.started": "2022-04-04T20:39:06.563062Z"
    },
    "papermill": {
     "duration": 0.049711,
     "end_time": "2022-04-04T20:41:15.806331",
     "exception": false,
     "start_time": "2022-04-04T20:41:15.756620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan 2015 Record for the  0 th cluster number of 10min intavels with zero pickups:  34\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  1 th cluster number of 10min intavels with zero pickups:  36\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  2 th cluster number of 10min intavels with zero pickups:  386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  3 th cluster number of 10min intavels with zero pickups:  49\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  4 th cluster number of 10min intavels with zero pickups:  23\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  5 th cluster number of 10min intavels with zero pickups:  37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  6 th cluster number of 10min intavels with zero pickups:  35\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  7 th cluster number of 10min intavels with zero pickups:  41\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  8 th cluster number of 10min intavels with zero pickups:  149\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  9 th cluster number of 10min intavels with zero pickups:  197\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  10 th cluster number of 10min intavels with zero pickups:  44\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  11 th cluster number of 10min intavels with zero pickups:  36\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  12 th cluster number of 10min intavels with zero pickups:  40\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  13 th cluster number of 10min intavels with zero pickups:  42\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  14 th cluster number of 10min intavels with zero pickups:  26\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  15 th cluster number of 10min intavels with zero pickups:  51\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  16 th cluster number of 10min intavels with zero pickups:  35\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  17 th cluster number of 10min intavels with zero pickups:  39\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  18 th cluster number of 10min intavels with zero pickups:  32\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  19 th cluster number of 10min intavels with zero pickups:  33\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  20 th cluster number of 10min intavels with zero pickups:  662\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  21 th cluster number of 10min intavels with zero pickups:  32\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  22 th cluster number of 10min intavels with zero pickups:  29\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  23 th cluster number of 10min intavels with zero pickups:  30\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  24 th cluster number of 10min intavels with zero pickups:  31\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  25 th cluster number of 10min intavels with zero pickups:  36\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  26 th cluster number of 10min intavels with zero pickups:  64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  27 th cluster number of 10min intavels with zero pickups:  39\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  28 th cluster number of 10min intavels with zero pickups:  117\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  29 th cluster number of 10min intavels with zero pickups:  43\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  30 th cluster number of 10min intavels with zero pickups:  328\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  31 th cluster number of 10min intavels with zero pickups:  38\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  32 th cluster number of 10min intavels with zero pickups:  58\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  33 th cluster number of 10min intavels with zero pickups:  29\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jan 2015 Record for the  34 th cluster number of 10min intavels with zero pickups:  36\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for each cluster number of 10min intravels with 0 pickups\n",
    "for i in range(35):\n",
    "    print(\"Jan 2015 Record for the \",i,\"th cluster number of 10min intavels with zero pickups: \",4464 - len(set(jan_2015_unique[i])))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a7e3c",
   "metadata": {
    "papermill": {
     "duration": 0.019982,
     "end_time": "2022-04-04T20:41:15.847044",
     "exception": false,
     "start_time": "2022-04-04T20:41:15.827062",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd28d33",
   "metadata": {
    "papermill": {
     "duration": 0.020083,
     "end_time": "2022-04-04T20:41:15.887576",
     "exception": false,
     "start_time": "2022-04-04T20:41:15.867493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff1a2f",
   "metadata": {
    "papermill": {
     "duration": 0.020017,
     "end_time": "2022-04-04T20:41:15.927919",
     "exception": false,
     "start_time": "2022-04-04T20:41:15.907902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.855662,
   "end_time": "2022-04-04T20:41:16.758557",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-04T20:40:54.902895",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
