{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Newyork city Taxi Demand Prediction Data Processing and modeling\n\n   Please review and visit  [NYC Taxi Demand EDA](https://www.kaggle.com/chiragtagadiya/nyc-taxi-demand-prediction-eda/edit) for Data Analysis.\n   \n   This Notebook cotains data preprocessing and Data Modeling\n   ","metadata":{}},{"cell_type":"markdown","source":"## Features in the dataset:\n<table border=\"1\">\n        <tr>\n            <th>Field Name</th>\n            <th>Description</th>\n        </tr>\n        <tr>\n            <td>VendorID</td>\n            <td>\n            A code indicating the TPEP provider that provided the record.\n            <ol>\n                <li>Creative Mobile Technologies</li>\n                <li>VeriFone Inc.</li>\n            </ol>\n            </td>\n        </tr>\n        <tr>\n            <td>tpep_pickup_datetime</td>\n            <td>The date and time when the meter was engaged.</td>\n        </tr>\n        <tr>\n            <td>tpep_dropoff_datetime</td>\n            <td>The date and time when the meter was disengaged.</td>\n        </tr>\n        <tr>\n            <td>Passenger_count</td>\n            <td>The number of passengers in the vehicle. This is a driver-entered value.</td>\n        </tr>\n        <tr>\n            <td>Trip_distance</td>\n            <td>The elapsed trip distance in miles reported by the taximeter.</td>\n        </tr>\n        <tr>\n            <td>Pickup_longitude</td>\n            <td>Longitude where the meter was engaged.</td>\n        </tr>\n        <tr>\n            <td>Pickup_latitude</td>\n            <td>Latitude where the meter was engaged.</td>\n        </tr>\n        <tr>\n            <td>RateCodeID</td>\n            <td>The final rate code in effect at the end of the trip.\n            <ol>\n                <li> Standard rate </li>\n                <li> JFK </li>\n                <li> Newark </li>\n                <li> Nassau or Westchester</li>\n                <li> Negotiated fare </li>\n                <li> Group ride</li>\n            </ol>\n            </td>\n        </tr>\n        <tr>\n            <td>Store_and_fwd_flag</td>\n            <td>This flag indicates whether the trip record was held in vehicle memory before sending to the vendor,<br> aka “store and forward,” because the vehicle did not have a connection to the server.<br><br>\n                Y= store and forward trip<br>\n                N= not a store and forward trip<br>\n            </td>\n        </tr>\n        <tr>\n            <td>Dropoff_longitude</td>\n            <td>Longitude where the meter was disengaged.</td>\n        </tr>\n        <tr>\n            <td>Dropoff_ latitude</td>\n            <td>Latitude where the meter was disengaged.</td>\n        </tr>\n        <tr>\n            <td>Payment_type</td>\n            <td>A numeric code signifying how the passenger paid for the trip.\n            <ol>\n                <li> Credit card </li>\n                <li> Cash </li>\n                <li> No charge </li>\n                <li> Dispute</li>\n                <li> Unknown </li>\n                <li> Voided trip</li>\n            </ol>\n            </td>\n        </tr>\n        <tr>\n            <td>Fare_amount</td>\n            <td>The time-and-distance fare calculated by the meter.</td>\n        </tr>\n        <tr>\n            <td>Extra</td>\n            <td>Miscellaneous extras and surcharges. Currently, this only includes. the $0.50 and $1 rush hour and overnight charges.</td>\n        </tr>\n        <tr>\n            <td>MTA_tax</td>\n            <td>0.50 MTA tax that is automatically triggered based on the metered rate in use.</td>\n        </tr>\n        <tr>\n            <td>Improvement_surcharge</td>\n            <td>0.30 improvement surcharge assessed trips at the flag drop. the improvement surcharge began being levied in 2015.</td>\n        </tr>\n        <tr>\n            <td>Tip_amount</td>\n            <td>Tip amount – This field is automatically populated for credit card tips.Cash tips are not included.</td>\n        </tr>\n        <tr>\n            <td>Tolls_amount</td>\n            <td>Total amount of all tolls paid in trip.</td>\n        </tr>\n        <tr>\n            <td>Total_amount</td>\n            <td>The total amount charged to passengers. Does not include cash tips.</td>\n        </tr>\n</table>","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd#similar to pandas\n\nimport pandas as pd#pandas to create small dataframes \n\n#!pip3 install folium\nimport folium #open street map\n\n# unix time: https://www.unixtimestamp.com/\nimport datetime #Convert to unix time\n\nimport time #Convert to unix time\n\nimport numpy as np#Do aritmetic operations on arrays\n\n# matplotlib: used to plot graphs\nimport matplotlib\n# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user intractive like zoom in and zoom out\nmatplotlib.use('nbagg')\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \n\n# this lib is used while we calculate the stight line distance between two (lat,lon) pairs in miles\n#!pip install gpxpy\nimport gpxpy.geo #Get the haversine distance\n\nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n\n\n# to install xgboost: \n# !conda install py-xgboost --yes\n#!pip3 install xgboost\n# if it didnt happen check install_xgboost.JPG\nimport xgboost as xgb\n\n# to install sklearn: pip install -U scikit-learn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-05T14:43:08.848945Z","iopub.execute_input":"2022-04-05T14:43:08.8494Z","iopub.status.idle":"2022-04-05T14:43:10.967426Z","shell.execute_reply.started":"2022-04-05T14:43:08.849354Z","shell.execute_reply":"2022-04-05T14:43:10.966627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing Pipiline\n","metadata":{}},{"cell_type":"code","source":"def convert_to_unix_timestamp(s):\n    return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\"\"\"\nReturned Columns:\n    # 1. passenger_count\n    # 2. trip_distance\n    # 3. pickup_longitude\n    # 4. dropoff_latitude\n    # 5. dropoff_longitude\n    # 6. pickup_latitude\n    # 7. total_amount : total paid fair amount\n    # 8. trip_times :  duration of each trip\n    # 9. pickup_times : pickup time converted into unix time \n    # 10.drop times: drop time converted to unix time\n    # 11.Speed : velocity of each trip\n\"\"\"\n\ndef create_dataframe_with_trip_times(df):\n    \n    duration_df = df[['tpep_pickup_datetime','tpep_dropoff_datetime']].compute()\n    # get pickup values in unix time\n    pickup_times = [convert_to_unix_timestamp(value) for value in duration_df['tpep_pickup_datetime'].values]\n    # get dropoff values in unix time\n    drop_times = [convert_to_unix_timestamp(value) for value in duration_df['tpep_dropoff_datetime'].values]\n    # drop times and pickup times devide by 60 will give minutes\n    trip_times = (np.array(drop_times) - np.array(pickup_times))/ float(60) \n    result_df = df[['passenger_count','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','total_amount']].compute()\n    \n    result_df['trip_times'] = trip_times\n    result_df['pickup_times'] = pickup_times\n    result_df['drop_times'] = drop_times\n    # velocity = distance/ duration\n    result_df['Speed'] = 60*(result_df['trip_distance']/result_df['trip_times'])\n    \n    return result_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_outliers(df):\n    print(\"Number of records in dataframe : {}\".format(df.shape[0]))\n    # remove Latitude and longitude errors and outliers\n    a = df.shape[0]\n    temp_frame = df[((df.dropoff_longitude >= -74.15) & (df.dropoff_longitude <= -73.7004) &\\\n                       (df.dropoff_latitude >= 40.5774) & (df.dropoff_latitude <= 40.9176)) & \\\n                       ((df.pickup_longitude >= -74.15) & (df.pickup_latitude >= 40.5774)& \\\n                       (df.pickup_longitude <= -73.7004) & (df.pickup_latitude <= 40.9176))]\n    b = temp_frame.shape[0]\n    print (\"Number of outlier coordinates lying outside NY boundaries:\",(a-b))\n    \n    \n    temp_frame = df[(df.trip_times > 0) & (df.trip_times < 720)]\n    c = temp_frame.shape[0]\n    print (\"Number of outliers from trip times analysis:\",(a-c))\n    \n    temp_frame = df[(df.Speed <= 65) & (df.Speed >= 0)]\n    e = temp_frame.shape[0]\n    print (\"Number of outliers from speed analysis:\",(a-e))\n    \n    temp_frame = df[(df.total_amount <1000) & (df.total_amount >0)]\n    f = temp_frame.shape[0]\n    print (\"Number of outliers from fare analysis:\",(a-f))\n\n    del temp_frame\n    \n    df = df[((df.dropoff_longitude >= -74.15) & (df.dropoff_longitude <= -73.7004) &\\\n                       (df.dropoff_latitude >= 40.5774) & (df.dropoff_latitude <= 40.9176)) & \\\n                       ((df.pickup_longitude >= -74.15) & (df.pickup_latitude >= 40.5774)& \\\n                       (df.pickup_longitude <= -73.7004) & (df.pickup_latitude <= 40.9176))]\n    \n    df = df[(df.trip_times > 0) & (df.trip_times < 720)]\n    df = df[(df.trip_distance > 0) & (df.trip_distance < 23)]\n    df = df[(df.Speed < 45.31) & (df.Speed > 0)]\n    df = df[(df.total_amount <1000) & (df.total_amount >0)]\n    \n    print (\"Total outliers removed\",a - df.shape[0])\n    print (\"---\"*50)\n    \n    return df\n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_regions(k, coords, frame_with_outliers_removed):\n    \"\"\"\n    Get the Cluster centroid and length of clusters, \n    Devide the NYC city into Different clusters based on number of pickups\n    \n    Parameters:\n    \n        k = number of clusters for Kmeans algorithm\n        coords =   Trip pick up location Latitude and Longitud  dataframe on which we want to clusters,\n        frame_with_outliers_removed = dataframe on which we want to fit clustering algorithm\n        \n    Returns:\n        \n        Number of region centroid\n    \n    \"\"\"\n    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=10000,random_state=42).fit(coords)\n    # predict the cluster and create new feature pickup clusters\n    frame_with_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_outliers_removed[['pickup_latitude', 'pickup_longitude']])\n    cluster_centroids = kmeans.cluster_centers_\n    cluster_len = len(cluster_centroids)\n    return cluster_centroids, cluster_len, frame_with_outliers_removed\n\n\n \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cluster_statistics(cluser_centroids, cluster_len):\n    \n    \"\"\"\n    Get the avg number  cluster with inter-cluster distance > 2\n    Get the avg number  cluster with inter-cluster distance < 2\n    Get the minimum inter-cluster distance ( our constraint for good clustering is 0.5 miles)\n    \n    \n    \n    Parameters:\n    \n        cluser_centroids = centroid of each cluster in city\n        cluster_len =  total number of clusters\n        \n    \"\"\"\n    \n    \n    print(\"Choosing the cluster of size : {} \".format(cluster_len))\n    less_than_2_miles=[]\n    more_than_2_miles=[]\n    # set min_dist with some large value (1000 miles)\n    min_dist = 1000\n    for i in  range(0,cluster_len) :\n        good_points=0\n        violated_points=0\n        for j in range(0, cluster_len):\n            if i != j :\n                # movable-type.co.uk/scripts/latlong.html\n                # haversine distance\n                centroid1 = cluser_centroids[i]\n                centroid2 = cluser_centroids[j]\n                distance_bw_centroids_in_meters = gpxpy.geo.haversine_distance(centroid1[0], centroid1[1],centroid2[0], centroid2[1])\n                # haversine_distance will be in meters, so convert distance into miles  by deviding with 1.60934*1000.\n                distance_bw_centroids_in_miles = distance_bw_centroids_in_meters/(1.60934*1000)\n                min_dist = min(min_dist,distance_bw_centroids_in_miles)\n                \n                if distance_bw_centroids_in_miles <=2:\n                    good_points+=1\n                else:\n                    violated_points+=1\n                    \n        less_than_2_miles.append(good_points)\n        more_than_2_miles.append(violated_points)\n        \n    \n    print(\"Final Result After Computation:\")\n    print(\"Avg Number of cluster with in vicinity [inter-cluster distance is < 2]  : {}\".format(np.ceil(sum(less_than_2_miles)/len(less_than_2_miles))))\n    print(\"Avg Number of cluster outside  vicinity [inter-cluster distance is >  2] : {}\".format(np.ceil(sum(more_than_2_miles)/len(more_than_2_miles))))\n    print(\"Minimum inter-cluster distance : {}\".format(min_dist))\n    print(\"+\"*70)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Refer:https://www.unixtimestamp.com/\n\n# 1420070400 : 2015-01-01 00:00:00 \n# 1422748800 : 2015-02-01 00:00:00 \n# 1425168000 : 2015-03-01 00:00:00\n# 1427846400 : 2015-04-01 00:00:00 \n# 1430438400 : 2015-05-01 00:00:00 \n# 1433116800 : 2015-06-01 00:00:00\n\n# 1451606400 : 2016-01-01 00:00:00 \n# 1454284800 : 2016-02-01 00:00:00 \n# 1456790400 : 2016-03-01 00:00:00\n# 1459468800 : 2016-04-01 00:00:00 \n# 1462060800 : 2016-05-01 00:00:00 \n# 1464739200 : 2016-06-01 00:00:00\n\n\n\ndef add_pickup_bins(frame,month,year):\n    \"\"\"\n    Devide all the trips in 10 minute bins    \n    \"\"\"\n    unix_pickup_times=[i for i in frame['pickup_times'].values]\n    #     unix_pickup_times = [i for i in frame['pickup_times'].values]\n    # index = 0 Jan_2015 Feb_2015 Mar_2015 Apr_2015 May_2015 June_2015\n    # index = 1 Jan_2015 Feb_2015 Mar_2015 Apr_2015 May_2015 June_2015\n    \n    unix_times = [[1420070400,1422748800,1425168000,1427846400,1430438400,1433116800],\\\n                    [1451606400,1454284800,1456790400,1459468800,1462060800,1464739200]]\n    \n    start_pickup_unix = unix_times[year-2015][month-1]\n    \n    # time.mktime will return timestamp in local time \n    # we will  subtract 24 ( 4 hours = 4*60*60 = 14400 seconds = 14400/600\n    # devide by 600 for 10 minutes inverval setting) =  24 \n    # if you are in Indian standard time then correction would be + 33 ,\n    # because India is GMT +5:30, 5.30*60*60 = 19800 => 19800/600 for 10 minutes bin we get +33.\n    \n    #     ten_minute_wise_binned_unix_pickup_times = [ int((i - start_pickup_unix )/600)+33  for i in unix_pickup_times]\n    #  I am in Eastern time so no correction needed. \n    correction = 0\n    ten_minute_wise_binned_unix_pickup_times=[(int((i-start_pickup_unix)/600) + correction) for i in unix_pickup_times]\n    frame['pickup_bins']=np.array(ten_minute_wise_binned_unix_pickup_times)\n    return frame\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# upto now we cleaned data and prepared data for the month 2015,\n\n# now do the same operations for months Jan, Feb, March of 2016\n\ndef dataPreparation(df,month, year, record_count ):\n    \"\"\"\n        Prepare final data.\n\n        1. get the correct column from dataframe\n        2. compute trip duration features\n        3. computer speed, unix time stamp for pickups, remove outliers\n        4. add pickup cluster for each trip\n        5. add pickup bin for each trip\n        6. groupby trip cluster, trip bin, and trip_distance \n        7. return prepared dataframe.\n    \"\"\"\n    # 1. get the correct column from dataframe 2. compute trip duration features\n    print(\"Adding Trip times for each trip \")\n    \n    df = create_dataframe_with_trip_times(df)\n    \n    # 3. computer speed, unix time stamp for pickups, remove outliers\n    print(\"Removing outliers from dataframe\")\n    \n    df = remove_outliers(df)\n    print(\"fraction of data points that remain after removing outliers\", float(len(df)/record_count))\n    \n    # 4. add pickup cluster for each trip\n    print(\"Creating Trip clusters\")\n    \n    # We have figure it out optimal value for kmeans clustering to 35 in eda section\n    \n    _,a, df = find_regions(35, df[['pickup_latitude', 'pickup_longitude']], df)\n    \n    print(\"Adding trip in 10 minute time interval bins\")\n    # 5.add pickup bin for each trip\n    df = add_pickup_bins(df,month,year)\n    \n    print(\"Group by trip cluster and then trip bin\")\n    # 6.groupby trip cluster, trip bin, and trip_distance \n    df_groupby = df[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()\n    \n    return df_groupby, df \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data files are extremely  large, more than 10 millions records\n# We already processed and saved processed file\n# So If you want to reproduce this uncomment below line\n\n# month_jan_2016 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2016-01.csv')\n# month_feb_2016 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2016-02.csv')\n# month_mar_2016 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2016-03.csv')\n# month_jan_2015 = dd.read_csv('../input/nyc-yellow-taxi-trip-data/yellow_tripdata_2015-01.csv')\n\n# jan_2015_groupby,jan_2015_frame = dataPreparation(month_jan_2015,1,2015,len(month_jan_2015))\n# jan_2016_groupby,jan_2016_frame = dataPreparation(month_jan_2016,1,2016,len(month_jan_2016))\n\n# feb_2016_groupby,feb_2016_frame = dataPreparation(month_feb_2016,2,2016,len(month_feb_2016))\n\n\n# mar_2016_groupby,mar_2016_frame = dataPreparation(month_mar_2016,3,2016,len(month_mar_2016))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Dataset\n\nfeb_2016_frame = dd.read_csv('../input/preprocessing-files/feb_2016_processed.csv')\njan_2015_frame  = dd.read_csv('../input/preprocessing-files/jan_month_2015.csv')\njan_2016_frame  = dd.read_csv('../input/preprocessing-files/jan_2016_processed.csv')\nmar_2016_frame  = dd.read_csv('../input/preprocessing-files/mar_2016_frame.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-05T14:43:13.837058Z","iopub.execute_input":"2022-04-05T14:43:13.837297Z","iopub.status.idle":"2022-04-05T14:43:14.042509Z","shell.execute_reply.started":"2022-04-05T14:43:13.837273Z","shell.execute_reply":"2022-04-05T14:43:14.041798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Smoothing\n\n\n* There are **some bins in dataset having 0 pickups**, we want to findout those bins and smooth the dataset, 0 values in pickups is problamatic for ratio features, we will smooth our dataset to overcome this issue\n","metadata":{}},{"cell_type":"code","source":"# Gets the unique bins where pickup values are present for each each reigion\n\n# for each cluster region we will collect all the indices of 10min intravels in which the pickups are happened\n# we got an observation that there are some pickpbins that doesnt have any pickups\ndef get_unique_pickup_bins(df, cluster_count = 35):\n    \"\"\"\n    For each cluster in dataset, find the number of unique time bins present in each cluster\n    \"\"\"\n    \n    values = []\n    for i in range(0,cluster_count):\n        new = df[df['pickup_cluster'] == i]\n        list_unq = list(set(new['pickup_bins']))\n        list_unq.sort()\n        values.append(list_unq)\n    return values\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T14:43:17.526219Z","iopub.execute_input":"2022-04-05T14:43:17.526732Z","iopub.status.idle":"2022-04-05T14:43:17.532409Z","shell.execute_reply.started":"2022-04-05T14:43:17.526697Z","shell.execute_reply":"2022-04-05T14:43:17.531662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for every month we get all indices of 10min intravels in which atleast one pickup got happened\n\nstart = time.time() \njan_2015_unique = get_unique_pickup_bins(jan_2015_group_frame)\n\n\n#jan 2016\njan_2016_unique = get_unique_pickup_bins(jan_2016_group_frame)\n\n# #feb\nfeb_2016_unique = get_unique_pickup_bins(feb_2016_group_frame)\n\n# #march\nmar_2016_unique = get_unique_pickup_bins(mar_2016_group_frame)\nend = time.time()\nprint(\"Total processing time\", end-start)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T14:43:21.241872Z","iopub.execute_input":"2022-04-05T14:43:21.242138Z","iopub.status.idle":"2022-04-05T14:43:28.442008Z","shell.execute_reply.started":"2022-04-05T14:43:21.24211Z","shell.execute_reply":"2022-04-05T14:43:28.441157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for each cluster number of 10min intravels with 0 pickups\nsum=0\nfor i in range(35):\n    print(\"Jan 2015 Record for the \",i,\"th cluster number of 10min intavels with zero pickups: \",4464 - len(set(jan_2015_unique[i])))\n    sum+= (4464 - len(set(jan_2015_unique[i])))\n    print('-'*100)\n    \nprint(\"Total Number of pick up bins missing in all the cluster combined {}\".format(sum))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:47:31.658607Z","iopub.execute_input":"2022-04-05T16:47:31.658904Z","iopub.status.idle":"2022-04-05T16:47:31.688012Z","shell.execute_reply.started":"2022-04-05T16:47:31.658861Z","shell.execute_reply":"2022-04-05T16:47:31.687258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We want to somehow fill those empty bins with some values using imputation. \n     There are multiple cases to handle, see details below\n\n     there are two ways to fill up these values\n     \n* **Total Number of pick up bins missing in all the cluster combined 2937**\n<ul>\n<li> Fill the missing value with 0's</li>\n<li> Fill the missing values with the avg values\n<ul>\n<li> Case 1:(values missing at the start)  <br>Ex1: \\_ \\_ \\_ x =>ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4) <br> Ex2: \\_ \\_ x => ceil(x/3), ceil(x/3), ceil(x/3) </li>\n<li> Case 2:(values missing in middle) <br>Ex1: x \\_ \\_ y => ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4) <br> Ex2: x \\_ \\_ \\_ y => ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5) </li>\n<li> Case 3:(values missing at the end)  <br>Ex1: x \\_ \\_ \\_  => ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4) <br> Ex2: x \\_  => ceil(x/2), ceil(x/2) </li>\n</ul>\n</li>\n</ul>\n\n    we will be using avg value imputation.\n    ","metadata":{}},{"cell_type":"code","source":"\n\n# for every 10min intravel(pickup_bin) we will check it is there in our unique bin,\n# if it is there we will add the count_values[index] to smoothed data\n# if not we add 0 to the smoothed data\n# we finally return smoothed data\ndef fill_missing(count_values,values):\n    \n    \"\"\"\n    Fills a value of zero for every bin where no pickup data is present \n    \n    Parameters:\n        count_values: number pickps that are happened in each region for each 10min intravel\n        values: number of unique bins\n        \n    Returns:\n        For each cluster fill 0 pickups for initially empty pickups which will be imputed with new values in next step\n    \n    \"\"\"\n    \n    smoothed_regions = []\n    \n    index = 0\n    for region in range(0, len(values)) :\n        smoothed_bins=[]\n        for  bin in range(4464):\n            if bin in values[region]:\n                smoothed_bins.append(count_values[index])\n                index+=1\n            else:\n                smoothed_bins.append(0)\n        smoothed_regions.extend(smoothed_bins)\n    return smoothed_regions\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:44:39.793857Z","iopub.execute_input":"2022-04-05T16:44:39.794685Z","iopub.status.idle":"2022-04-05T16:44:39.801541Z","shell.execute_reply.started":"2022-04-05T16:44:39.794636Z","shell.execute_reply":"2022-04-05T16:44:39.800551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jan_2015_group_frame = jan_2015_frame[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:18:18.33812Z","iopub.execute_input":"2022-04-05T15:18:18.338385Z","iopub.status.idle":"2022-04-05T15:18:18.350392Z","shell.execute_reply.started":"2022-04-05T15:18:18.338348Z","shell.execute_reply":"2022-04-05T15:18:18.349388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ndef smoothing(count_values, values):\n    \"\"\"\n    Fills a value of zero for every bin where no pickup data is present \n    \n    Parameters:\n        count_values: number pickps that are happened in each region for each 10min intravel\n        values: number of unique bins\n\n    Returns:\n        smoothed data (which is calculated based on the methods that are discussed in the above markdown cell)\n    \"\"\"\n    \n    \n    # stores the final smoothed pick up data for each regions\n    smoothed_regions = [] \n    index = 0\n    repeat=0\n    smoothed_value = 0\n    \n    for r in range(0,len(values)):\n        #stores the final smoothed values for each region we are iterating over\n        smoothed_data =  []\n        repeat=0\n        for i in range(4464):\n            if repeat!=0: # prevents iteration for a value which is already visited/resolved\n                repeat-=1\n                continue\n            if i in values[r]: # if pick up bin is present in current region\n                smoothed_data.append(count_values[index])\n            # pick up bin is not present in current region\n            # not imputation time\n            #   Case 1:(values missing at the start)\n            #       Ex1: \\_ \\_ \\_ x =>ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4)\n            #       Ex2: \\_ \\_ x => ceil(x/3), ceil(x/3), ceil(x/3)\n            #   Case 2:(values missing in middle)\n            #       Ex1: x \\_ \\_ y => ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4)\n            #       Ex2: x \\_ \\_ \\_ y => ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5)\n            #   Case 3:(values missing at the end)\n            #       Ex1: x \\_ \\_ \\_ => ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4)\n            #       Ex2: x \\_ => ceil(x/2), ceil(x/2)\n            else:\n                # first index is not missing in current region\n                # Case 2 or Case 3\n                if i !=0 :\n                    right_hand_limit=0\n                    for j in range(i,4464):\n                        if  j not in values[r]: #searches for the left-limit or the pickup-bin value which has a pickup value\n                            continue\n                        else:\n                            # we are in case 2\n                            # we found some values at right hand side of the current index\n                            right_hand_limit=j\n                            break\n                    # we did not find any right hand values with non zero pickup\n                    # case 3\n                    if right_hand_limit == 0:\n                        #Case 1: When we have the last/last few values are found to be missing,hence we have no right-limit here\n                        \n                        # +2 if we have only last value missing ( for index 4463 is missing then we will impite it by using 4462 value\n#                         devide by 2)\n                        smoothed_value = count_values[index-1] *1.0 / ((4464-i)+2)*1.0\n                        for j in range(i,4464):\n                            smoothed_data.append(math.ceil(smoothed_value))\n                        smoothed_data[i-1] = math.ceil(smoothed_value)\n                        repeat=(4463-i)\n                        index-=1\n                    else:\n                        #Case 2: When we have the missing values between two known values\n                        smoothed_value=(count_values[index-1]+count_values[index])*1.0/((right_hand_limit-i)+2)*1.0\n                        \n                        for j in range(i,right_hand_limit+1):\n                            smoothed_data.append(math.ceil(smoothed_value))\n                        smoothed_data[i-1] = math.ceil(smoothed_value)\n                        repeat=(right_hand_limit-i)\n                else:\n                    #Case 3: When we have the first/first few values are found to be missing,\n                    # hence we have no left-limit here\n                    right_hand_limit=0\n                    for j in range(i,4464):\n                        if  j not in values[r]: #searches for the left-limit or the pickup-bin value which has a pickup value\n                            continue\n                        else:\n                            right_hand_limit=j\n                            break\n                    smoothed_value = count_values[index] *1.0 / ((right_hand_limit-i)+1)*1.0\n                    for j in range(i,right_hand_limit+1):\n                            smoothed_data.append(math.ceil(smoothed_value))\n                    repeat=(right_hand_limit-i)\n                    \n            index+=1\n        smoothed_regions.extend(smoothed_data)\n        \n    return smoothed_regions\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:44:05.123782Z","iopub.execute_input":"2022-04-05T16:44:05.124342Z","iopub.status.idle":"2022-04-05T16:44:05.138476Z","shell.execute_reply.started":"2022-04-05T16:44:05.124306Z","shell.execute_reply":"2022-04-05T16:44:05.137926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filling Missing values of Jan-2015 with 0\n# here in jan_2015_groupby dataframe the trip_distance represents the number of pickups that are happened\njan_2015_fill_with_zero = fill_missing(jan_2015_group_frame['trip_distance'].compute().values, jan_2015_unique)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:44:44.61587Z","iopub.execute_input":"2022-04-05T16:44:44.61644Z","iopub.status.idle":"2022-04-05T16:44:57.820378Z","shell.execute_reply.started":"2022-04-05T16:44:44.616401Z","shell.execute_reply":"2022-04-05T16:44:57.819143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Smoothing Missing values of Jan-2015\njan_2015_smooth = smoothing(jan_2015_group_frame['trip_distance'].compute().values,jan_2015_unique)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:44:08.311685Z","iopub.execute_input":"2022-04-05T16:44:08.312437Z","iopub.status.idle":"2022-04-05T16:44:22.283942Z","shell.execute_reply.started":"2022-04-05T16:44:08.312399Z","shell.execute_reply":"2022-04-05T16:44:22.283061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of empty bins we added value are {}\".format( len(jan_2015_smooth) - len(jan_2015_group_frame['trip_distance'].compute().values)))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:50:18.987952Z","iopub.execute_input":"2022-04-05T16:50:18.98826Z","iopub.status.idle":"2022-04-05T16:50:28.841632Z","shell.execute_reply.started":"2022-04-05T16:50:18.988229Z","shell.execute_reply":"2022-04-05T16:50:28.841076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hours in day * months in day *  number of minutes in hour / interval length(10 minute)\n# number of 10min indices for jan 2015= 24*31*60/10 = 4464\n# number of 10min indices for jan 2016 = 24*31*60/10 = 4464\n# number of 10min indices for feb 2016 = 24*29*60/10 = 4176\n# number of 10min indices for march 2016 = 24*30*60/10 = 4320\n# for each cluster we will have 4464 values, therefore 35*4464 = 156240 (length of the jan_2015_fill)\nprint(\"number of 10min intravels among all the clusters \",len(jan_2015_fill_with_zero))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:44:57.821982Z","iopub.execute_input":"2022-04-05T16:44:57.822224Z","iopub.status.idle":"2022-04-05T16:44:57.827244Z","shell.execute_reply.started":"2022-04-05T16:44:57.822195Z","shell.execute_reply":"2022-04-05T16:44:57.826395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Smoothing vs filling","metadata":{}},{"cell_type":"code","source":"# Smoothing vs Filling\n# sample plot that shows two variations of filling missing values\n# we have taken the number of pickups for cluster region 2\n%matplotlib inline\nj=0\nfor i in range(0,len(jan_2015_unique)):\n    plt.figure(figsize=(10,5))\n    plt.plot(jan_2015_fill_with_zero[i*4464:j+4464], label=\"zero filled values for cluster {}\".format(i))\n    plt.plot(jan_2015_smooth[i*4464:j+4464], label=\"filled with avg values for cluster {}\".format(i))\n    plt.legend()\n    plt.title(\"for cluster {} we filled {} bins \".format(i,4464 - len(set(jan_2015_unique[i]))))\n    plt.show()\n    j+=4464","metadata":{"execution":{"iopub.status.busy":"2022-04-05T17:03:42.287172Z","iopub.execute_input":"2022-04-05T17:03:42.287408Z","iopub.status.idle":"2022-04-05T17:03:50.586826Z","shell.execute_reply.started":"2022-04-05T17:03:42.287383Z","shell.execute_reply":"2022-04-05T17:03:50.586153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# why we choose, these methods and which method is used for which data?\n\n# Ans: consider we have data of some month in 2015 jan 1st, 100 _ _ _ 150, i.e there are 100 pickups that are happened in 1st \n# st 10min intravel, 0 pickups happened in 2nd 10mins intravel, 0 pickups happened in 3rd 10min intravel \n# and 150 pickups happened in 4th 10min intravel.\n# in fill_missing method we replace these values like 100, 0, 0, 140\n# where as in smoothing method we replace these values as 60,60,60,60 if you can check the number of pickups \n# that are happened in the first 40min are same in both cases, but if you can observe that we looking at the future values \n# wheen you are using smoothing we are looking at the future number of pickups which might cause a data leakage.\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remark:\n    we use smoothing only for training data to avoid data leakage.\n    and we use simple fill_misssing method for 2016th data (Which will be used for testing)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}